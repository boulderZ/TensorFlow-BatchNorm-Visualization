{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Batch Normalization and Visualization in TensorFlow with TensorBoard\n",
    "=============\n",
    "\n",
    "Work in Progress. Planning to use the higher level code in tf.slim to implement the models\n",
    "------------\n",
    "\n",
    "Using notMNiST dataset and starter code from Udacity ud730 as starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_datasetju\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Define a model for experimenting with. Will use a 10 layer simple net with 500 neurons each with RELU activation. Note this is not likely a good choice for this notMNIST problem, but is chosen because it should demonstrate the effectiveness of batch normalization. It also serves as a sanity check on the implementation of batch normalization. Here I am using the examples from Stanford class cs231n on convolutional neural nets http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf\n",
    "as reference material. In those lectures, they show that the gradients will vanish without proper initialization. They show some examples of different initializations and how the activations either saturate or go to zero. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background on how to use batch_norm()\n",
    "#### Using `update_collections`\n",
    "\n",
    "Another post from @squada suggesting to use `update_collections=None`:\n",
    "```python\n",
    "slim = tf.contrib.slim\n",
    "def model(data, is_training=False, reuse=None, scope='my_model'):\n",
    "  # Define a variable scope to contain all the variables of your model\n",
    "  with tf.variable_scope(scope, 'model', data, reuse=reuse):  # I had to edit this and remove 'model' to make it work\n",
    "    # Configure arguments of fully_connected layers\n",
    "    with slim.arg_scope([slim.fully_connected],\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        normalizer_fn=slim.batch_nom):\n",
    "      # Configure arguments of batch_norm layers\n",
    "      with slim.arg_scope([slim.batch_norm],\n",
    "                          decay=0.9,  # Adjust decay to the number of iterations\n",
    "                          update_collections=None, # Make sure updates happen automatically\n",
    "                          is_training=is_training, # Switch behavior from training to non-training):\n",
    "        net = slim.fully_connected(data, 100, scope='fc1')\n",
    "        net = slim.fully_connected(net, 200, scope='fc2')\n",
    "        ....\n",
    "        # Don't use activation_fn nor batch_norm in the last layer        \n",
    "        net = slim.fully_connected(net, 10, activation_fn=None, normalizer_fn=None, scope='fc10')\n",
    "       return net\n",
    "```\n",
    " \n",
    "A working example on using batch_norm() that used `update_collections=None` is \n",
    "[working examples](http://stackoverflow.com/documentation/tensorflow/7909/using-batch-normalization/25676/a-full-working-example-of-2-layer-neural-network-with-batch-normalization-mnist#t=20170508203945378031)\n",
    "\n",
    "Another example using `updates_collections=ops.GraphKeys.UPDATE_OPS` is explained [A Gentle...](http://ruishu.io/2016/12/27/batchnorm/) and code on GitHub at [RuiShu](https://github.com/RuiShu/micro-projects/blob/master/tf-batchnorm-guide/batchnorm_guide.ipynb)\n",
    "\n",
    "There is a speed penalty for using `updates_collections=None` for deep networks. With it set to `None`, it does the calculations in place rather than allowing them to be collected and optimally computed later. \n",
    "\n",
    "\n",
    "Finally latest docs show method for using `updates_collections=ops.GraphKeys.UPDATE_OPS`\n",
    "\n",
    "Note: When is_training is True the moving_mean and moving_variance need to be\n",
    "    updated, by default the update_ops are placed in `tf.GraphKeys.UPDATE_OPS` so\n",
    "    they need to be added as a dependency to the `train_op`, example:\n",
    "\n",
    "```python\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss)\n",
    "````\n",
    "#### Setting `decay`\n",
    "The setting of the `decay` value is critical if running small batches. The default value of `0.999` can result in poor results with small batches. \n",
    "From same thread as above:\n",
    "\"We could change the default to 0.9 or document better its impact in smaller datasets or few updates.\n",
    "@vincentvanhoucke in our distributed setting we usually do millions of updates so it is ok, however in other cases like the one here which does only a few hundreds of updates it makes a big difference:\n",
    "For example using decay=0.999 has a 0.36 bias after 1000 updates, but that bias goes down to 0.000045 after 10000 updates and to 0.0 after 50000 updates.\"\n",
    "\n",
    "#### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the inputs and defaults for fully_connected() and batch_norm() area shown below. \n",
    "\n",
    "def fully_connected(inputs,\n",
    "                    num_outputs,\n",
    "                    activation_fn=nn.relu,\n",
    "                    normalizer_fn=None,\n",
    "                    normalizer_params=None,\n",
    "                    weights_initializer=initializers.xavier_initializer(),\n",
    "                    weights_regularizer=None,\n",
    "                    biases_initializer=init_ops.zeros_initializer(),\n",
    "                    biases_regularizer=None,\n",
    "                    reuse=None,\n",
    "                    variables_collections=None,\n",
    "                    outputs_collections=None,\n",
    "                    trainable=True,\n",
    "                    scope=None):\n",
    "                    \n",
    "def batch_norm(inputs,\n",
    "               decay=0.999,\n",
    "               center=True,\n",
    "               scale=False,\n",
    "               epsilon=0.001,\n",
    "               activation_fn=None,\n",
    "               param_initializers=None,\n",
    "               param_regularizers=None,\n",
    "               updates_collections=ops.GraphKeys.UPDATE_OPS,\n",
    "               is_training=True,\n",
    "               reuse=None,\n",
    "               variables_collections=None,\n",
    "               outputs_collections=None,\n",
    "               trainable=True,\n",
    "               batch_weights=None,\n",
    "               fused=False,\n",
    "               data_format=DATA_FORMAT_NHWC,\n",
    "               zero_debias_moving_mean=False,\n",
    "               scope=None,\n",
    "               renorm=False,\n",
    "               renorm_clipping=None,\n",
    "               renorm_decay=0.99):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the flow is modeled from my code used for Udacity Self Driving Car project on German Traffic Sign classification [my code](https://github.com/boulderZ/German-Traffic-Sign-Classification).\n",
    "Start by defining a 10 layer net. Below I am taking advantage of tf.contrib.slim to get access to higher level features and to use arg_scope(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define simple ten layer net with optional batch normalization (BN)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()  ### NOTE: This is needed in jupyter notebooks when using tf.get_variable(), otherwise \n",
    "                          ### when you run a second time, you will get errors that variables already exist.\n",
    "slim = tf.contrib.slim\n",
    "updates_collections=tf.GraphKeys.UPDATE_OPS\n",
    "#updates_collections = None   # uncomment this to do the batch norm updates in place\n",
    "normalizer_fn=slim.batch_norm\n",
    "#normalizer_fn = None   # uncomment this to remove batch normalization\n",
    "#weights_initializer=initializers.xavier_initializer() ## using this without BN, will also work well\n",
    "weights_initializer = tf.random_normal_initializer(stddev=.01) # this is a bad initializer on purpose to test BN\n",
    "## try setting normalizer_fn = None and weights_initializer = tf.random_normal_initializer(stddev=.01)\n",
    "## you should see that the training fails and resulting accuracy is random (0.1)\n",
    "## Then same poor initializer but with normalizer_fn=slim.batch_norm and results are quite good (accuracy ~0.9)\n",
    "\n",
    "def model(data, is_training=False, reuse=None, scope='model'):\n",
    "    # Define a variable scope to contain all the variables of your model\n",
    "    with tf.variable_scope(scope, data, reuse=reuse):  \n",
    "        # Configure arguments of fully_connected layers\n",
    "        with slim.arg_scope([slim.fully_connected],\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        weights_initializer=weights_initializer,\n",
    "                        normalizer_fn=normalizer_fn):\n",
    "            # Configure arguments of batch_norm layers\n",
    "            with slim.arg_scope([slim.batch_norm],\n",
    "                          decay=0.9,  # Adjust decay to the number of iterations\n",
    "                          updates_collections=updates_collections, # None: Make sure updates happen automatically\n",
    "                          is_training=is_training): # Switch behavior from training to non-training):\n",
    "                net = slim.fully_connected(data, 500, scope='fc1')\n",
    "                tf.summary.histogram('fc1_activations', net)       # add visualization of activations for TensorBoard\n",
    "                net = slim.fully_connected(net, 500, scope='fc2')\n",
    "                net = slim.fully_connected(net, 500, scope='fc3')\n",
    "                net = slim.fully_connected(net, 500, scope='fc4')\n",
    "                net = slim.fully_connected(net, 500, scope='fc5')\n",
    "                tf.summary.histogram('fc5_activations', net)\n",
    "                net = slim.fully_connected(net, 500, scope='fc6')\n",
    "                net = slim.fully_connected(net, 500, scope='fc7')\n",
    "                net = slim.fully_connected(net, 500, scope='fc8')\n",
    "                net = slim.fully_connected(net, 500, scope='fc9')\n",
    "                tf.summary.histogram('fc9_activations', net)\n",
    "\n",
    "                # Don't use activation_fn nor batch_norm in the last layer        \n",
    "                logits = slim.fully_connected(net, 10, activation_fn=None, normalizer_fn=None, scope='fc10')\n",
    "                tf.summary.histogram('fc10_activations', logits)\n",
    "                \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, image_size*image_size))\n",
    "y = tf.placeholder(tf.int32, (None,10))\n",
    "#one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### IMPORTS needed to use tf.contrib.layers.optimize_loss()\n",
    "\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import init_ops\n",
    "global_step = variable_scope.get_variable(  # this needs to be defined for tf.contrib.layers.optimize_loss()\n",
    "      \"global_step\", [],\n",
    "      trainable=False,\n",
    "      dtype=dtypes.int64,\n",
    "      initializer=init_ops.constant_initializer(0, dtype=dtypes.int64))\n",
    "\n",
    "rate = 0.001 \n",
    "logits = model(x,is_training)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "tf.summary.scalar('loss', loss_operation)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "#training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "## Experiment with optimize_loss() which does summaries on gradients for TensorBoard. \n",
    "if updates_collections: # if not None, then update the moving_mean and moving_variance\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        training_operation = tf.contrib.layers.optimize_loss(\n",
    "                    loss_operation, global_step, learning_rate=rate, optimizer='Adam',summaries=[\"gradients\"])\n",
    "else:\n",
    "    training_operation = tf.contrib.layers.optimize_loss(\n",
    "              loss_operation, global_step, learning_rate=rate, optimizer='Adam',summaries=[\"gradients\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))  # returns tensor of dtype = boolean\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  # convert from boolean to float32\n",
    "saver = tf.train.Saver()\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y,keep_prob: 1.0,\n",
    "                                                          is_training: False})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "#print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.873\n",
      "Training Loss =  0.689682\n",
      "Training Accuracy =  0.8741\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.889\n",
      "Training Loss =  0.626996\n",
      "Training Accuracy =  0.8932\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./lenet'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from datetime import datetime\n",
    "EPOCHS = 2  # EPOCHS = 2 \n",
    "#logdir = FLAGS.train_dir + '/' + datetime.now().strftime('%Y%m%d-%H%M%S') + '/'\n",
    "logdir='tf_logs/'  # directory to save summaries in for TensorBoard\n",
    "\n",
    "# rename variables to match existing code\n",
    "X_train,y_train = train_dataset, train_labels\n",
    "X_validation,y_validation = valid_dataset, valid_labels\n",
    "X_test,y_test = test_dataset, test_labels\n",
    "\n",
    "# uncomment below to see overfitting. Also is a sanity check on the model/code. You should see training accuracy\n",
    "# of 1.0 with enough EPOCHS (EPOCHS=10,train_datset[:25]). Also, the first loss value should be 2.3 which is log(10)\n",
    "#X_train,y_train = train_dataset[:25], train_labels[:25]  # reduce training set (and increase EPOCHS) to see overfitting\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #summary_writer = tf.summary.FileWriter(logdir, sess.graph)  # Need for TensorBoard to save summaries \n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            #sess.run(training_operation, feed_dict={x: batch_x, y: batch_y,keep_prob: 1.0})\n",
    "            _, loss = sess.run([training_operation,loss_operation], feed_dict={x: batch_x, y: batch_y,keep_prob: 1.0,\n",
    "                                                                              is_training: True})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        training_accuracy = evaluate(X_train,y_train)\n",
    "        ## add code to save summaries for TensorBoard\n",
    "        str_epoch = logdir + 'run_' + str(i) # new directory for each run for tensorboard, then you can view by run(epoch)\n",
    "        summary_writer = tf.summary.FileWriter(str_epoch, sess.graph) \n",
    "        summary_str = sess.run(summary, feed_dict={x: batch_x, y: batch_y,keep_prob: 1.0,\n",
    "                                                  is_training: False})\n",
    "        summary_writer.add_summary(summary_str, i)\n",
    "        summary_writer.flush()  # evidently this is needed sometimes or scalars will not show up on tensorboard.\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print('Training Loss = ',loss)\n",
    "        print('Training Accuracy = ',training_accuracy)\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./lenet\n",
      "Test Accuracy = 0.938\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
